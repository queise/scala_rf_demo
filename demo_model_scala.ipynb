{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils._\n",
    "import org.apache.spark.mllib.linalg.Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking com.databricks:spark-csv_2.11:1.4.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree_add_deps8414302016316025609/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree_add_deps8414302016316025609/https/repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar\n",
      "-> New file at /tmp/toree_add_deps8414302016316025609/https/repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.4.0/spark-csv_2.11-1.4.0.jar\n",
      "-> New file at /tmp/toree_add_deps8414302016316025609/https/repo1.maven.org/maven2/com/univocity/univocity-parsers/1.5.1/univocity-parsers-1.5.1.jar\n"
     ]
    }
   ],
   "source": [
    "%AddDeps com.databricks spark-csv_2.11 1.4.0 --transitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlC = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val fName = \"file:/home/jordi/Projects/demos/mock_NPPmodel_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+---------+------+------------+\n",
      "|Mileage|Age|Power|     Body|  Fuel|VehicleValue|\n",
      "+-------+---+-----+---------+------+------------+\n",
      "|  59300| 30|   85|  Station|DIESEL|       10209|\n",
      "|  81980| 24|  130|    Coupe|DIESEL|       17367|\n",
      "|  78480| 36|  120|Offroader|DIESEL|       18983|\n",
      "|   8230| 12|  110|Limousine|BENZIN|       21419|\n",
      "|   3480| 12|  130|    Coupe|BENZIN|       26788|\n",
      "|  24180|  6|  145|Smallsize|DIESEL|       36965|\n",
      "|   6430|  6|  120|Limousine|DIESEL|       37712|\n",
      "|  23680|  6|  130|Smallsize|BENZIN|       26880|\n",
      "|  16480| 12|  130|Smallsize|BENZIN|       32596|\n",
      "|  25799| 12|  185|Offroader|DIESEL|       47805|\n",
      "+-------+---+-----+---------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var df = sqlC.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \";\").load(fName)\n",
    "// select only the columns needed by the model:\n",
    "df = df.select(\"Mileage\", \"Age\", \"Power\", \"Body\", \"Fuel\", \"VehicleValue\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|          Mileage|               Age|             Power|      VehicleValue|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|            23840|             23840|             23840|             23840|\n",
      "|   mean|41915.34110738255|18.703439597315437|121.32151845637584|19967.896140939596|\n",
      "| stddev|51710.27847223828| 13.92402864403154|47.374738992271546|10848.195784641135|\n",
      "|    min|                5|                 6|                31|              1643|\n",
      "|    max|          3054512|               138|               468|            144498|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-----------+-----+\n",
      "|       Body|count|\n",
      "+-----------+-----+\n",
      "|    Compact|  106|\n",
      "|      Coupe| 3226|\n",
      "|  Smallsize| 5479|\n",
      "|  Limousine| 8923|\n",
      "|Convertable| 1912|\n",
      "|  Offroader| 2022|\n",
      "|    Station| 2172|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Body\").groupBy(\"Body\").count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows train set: 19032\n",
      "num rows test set :  4808\n"
     ]
    }
   ],
   "source": [
    "var Array(dfTrain, dfTest) = df.randomSplit(Array(0.8, 0.2), seed = 13)\n",
    "println(\"num rows train set: \" + dfTrain.count())\n",
    "println(\"num rows test set :  \" + dfTest.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline with training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// the label column (response) must be double float and must be named \"label\":\n",
    "dfTrain = dfTrain.withColumn(\"label\", dfTrain.col(\"VehicleValue\").cast(DoubleType)).drop(\"VehicleValue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val namesColsFeatures = List(\"Mileage\", \"Age\", \"Power\", \"Body\", \"Fuel\")\n",
    "val nameColsFeaturesStrings = Array(\"Body\", \"Fuel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+--------+\n",
      "|     Body|Body_idx|  Fuel|Fuel_idx|\n",
      "+---------+--------+------+--------+\n",
      "|Limousine|     0.0|BENZIN|     0.0|\n",
      "|    Coupe|     2.0|BENZIN|     0.0|\n",
      "|Limousine|     0.0|DIESEL|     1.0|\n",
      "|    Coupe|     2.0|BENZIN|     0.0|\n",
      "|    Coupe|     2.0|BENZIN|     0.0|\n",
      "+---------+--------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val index_transformers: Array[org.apache.spark.ml.PipelineStage] = nameColsFeaturesStrings.map(\n",
    "  colName => new StringIndexer()\n",
    "    .setInputCol(colName)\n",
    "    .setOutputCol(s\"${colName}_idx\")\n",
    "    .setHandleInvalid(\"error\") // options: \"skip\", \"error\". With \"skip\", when the transformer is applied to data that has rows with new levels, the resulting df will skip those rows\n",
    ")\n",
    "new Pipeline().setStages(index_transformers).fit(dfTrain).transform(dfTrain).select(\"Body\", \"Body_idx\", \"Fuel\", \"Fuel_idx\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+-------------+\n",
      "|Body_idx|     Body_vec|Fuel_idx|     Fuel_vec|\n",
      "+--------+-------------+--------+-------------+\n",
      "|     0.0|(6,[0],[1.0])|     0.0|(1,[0],[1.0])|\n",
      "|     2.0|(6,[2],[1.0])|     0.0|(1,[0],[1.0])|\n",
      "|     0.0|(6,[0],[1.0])|     1.0|    (1,[],[])|\n",
      "|     2.0|(6,[2],[1.0])|     0.0|(1,[0],[1.0])|\n",
      "|     2.0|(6,[2],[1.0])|     0.0|(1,[0],[1.0])|\n",
      "+--------+-------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val one_hot_encoders: Array[org.apache.spark.ml.PipelineStage] = nameColsFeaturesStrings.map(\n",
    "  colName => new OneHotEncoder()\n",
    "    .setInputCol(s\"${colName}_idx\")\n",
    "    .setOutputCol(s\"${colName}_vec\")\n",
    ")\n",
    "new Pipeline().setStages(index_transformers++one_hot_encoders).fit(dfTrain).transform(dfTrain).select(\"Body_idx\", \"Body_vec\", \"Fuel_idx\", \"Fuel_vec\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Mileage\", \"Age\", \"Power\", \"Body_vec\", \"Fuel_vec\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Create pipeline:\n",
    "val pipelineDataTransform = new Pipeline().setStages(index_transformers ++ one_hot_encoders ++ Array(assembler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fit pipeline with training data:\n",
    "val pipelineDataTransformFitted: PipelineModel = pipelineDataTransform.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Save pipeline to HDFS:\n",
    "pipelineDataTransformFitted.write.overwrite().save(\"storage/Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Transform data:\n",
    "dfTrain = pipelineDataTransformFitted.transform(dfTrain).select(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|  label|            features|\n",
      "+-------+--------------------+\n",
      "|17643.0|(10,[0,1,2,3,9],[...|\n",
      "|21158.0|(10,[0,1,2,5,9],[...|\n",
      "|35256.0|(10,[0,1,2,3],[11...|\n",
      "|14437.0|(10,[0,1,2,5,9],[...|\n",
      "| 5708.0|(10,[0,1,2,5,9],[...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Transform df to an RDD[LabeledPoint]:\n",
    "val rddTrain = dfTrain.map(row => LabeledPoint(row.getAs[Double](\"label\"), row(1).asInstanceOf[Vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the **Random Forest**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Set here specific parameters for the algorithm, otherwise just use the default ones form analytics.properties:\n",
    "val categoricalFeaturesInfo = Map[Int, Int]() // ((indexCategoricalColumn, numDistinctCategories)) // or several:  Map[Int, Int]((1,3),(2,5)) The map tells us that feature with index 1 has 3 levels, and feature with index 2 has 5 levels\n",
    "var numTrees = 10\n",
    "var featureSubsetStrategy = \"auto\" // Number of features to consider for splits at each node. Supported: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\". If \"auto\" is set, this parameter is set based on numTrees: if numTrees == 1, set to \"all\"; if numTrees > 1 (forest) set to \"sqrt\" for classification and to \"onethird\" for regression.\n",
    "var impurity = \"variance\"\n",
    "var maxDepth = 30 // max is 30\n",
    "var maxBins = 50\n",
    "val seedNum = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Train model using the random forest algorithm:\n",
    "val trainedRFmodel = RandomForest.trainRegressor(rddTrain, categoricalFeaturesInfo,\n",
    "numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save model to HDFS:\n",
    "trainedRFmodel.save(sc, \"storage/RFmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+---------+------+------------+\n",
      "|Mileage|Age|Power|     Body|  Fuel|VehicleValue|\n",
      "+-------+---+-----+---------+------+------------+\n",
      "|      5|  6|   85|Limousine|BENZIN|       17643|\n",
      "|     11|  6|  120|Limousine|DIESEL|       36762|\n",
      "|     11| 18|  155|    Coupe|BENZIN|       12052|\n",
      "|     11| 24|   65|Limousine|BENZIN|        6110|\n",
      "|     11| 24|   75|Limousine|BENZIN|        5347|\n",
      "+-------+---+-----+---------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTest = dfTest.select(\"Mileage\", \"Age\", \"Power\", \"Body\", \"Fuel\", \"VehicleValue\")\n",
    "val dfTest0 = dfTest \n",
    "dfTest.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Load pipeline and RF model from HDFS:\n",
    "val pipelineDataTransformFittedLoaded = PipelineModel.load(\"storage/Pipeline\")\n",
    "val trainedRFmodel = RandomForestModel.load(sc, \"storage/RFmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,9],[...|\n",
      "|(10,[0,1,2,3],[11...|\n",
      "|(10,[0,1,2,5,9],[...|\n",
      "|(10,[0,1,2,3,9],[...|\n",
      "|(10,[0,1,2,3,9],[...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Transform data:\n",
    "dfTest = pipelineDataTransformFittedLoaded.transform(dfTest).select(\"features\")\n",
    "dfTest.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+---------+------+------------+------------------+\n",
      "|Mileage|Age|Power|     Body|  Fuel|VehicleValue|  predVehicleValue|\n",
      "+-------+---+-----+---------+------+------------+------------------+\n",
      "|  62840| 24|  105|Limousine|DIESEL|       14802|11463.168594540179|\n",
      "|   9631| 12|  200|Limousine|DIESEL|       36556| 36023.64761904762|\n",
      "| 140470| 36|  130|Smallsize|DIESEL|        6660|11422.769262543354|\n",
      "|  63492| 36|  165|Limousine|DIESEL|       14328|20916.493130392355|\n",
      "|  67980| 18|   95|Smallsize|DIESEL|       17201|17847.394139007585|\n",
      "+-------+---+-----+---------+------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Transform df to an RDD[LabeledPoint]:\n",
    "val rddTest = dfTest.map(row => row(0).asInstanceOf[Vector])\n",
    "// Compute predictions and transform to array:\n",
    "val predictions = rddTest.map { features => trainedRFmodel.predict(features) }.collect()\n",
    "// create the rows that will constitute the new Df:\n",
    "val rows = dfTest0.rdd.zipWithIndex.map(_.swap).join(sc.parallelize(predictions).zipWithIndex.map(_.swap)).values.map { case (row: Row, x: Double) => Row.fromSeq(row.toSeq :+ x) }\n",
    "// create the new Df:\n",
    "val dfResult = sqlC.createDataFrame(rows, dfTest0.schema.add(\"predVehicleValue\", DoubleType, false))\n",
    "dfResult.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// create N (train, test) pairs:\n",
    "val folds = kFold(rddTrain, numFolds = 5, seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// train models and create (model, test) pairs\n",
    "val modelsAndTest = folds.map { case (train, test) => (RandomForest.trainRegressor(train, categoricalFeaturesInfo,\n",
    "  10, featureSubsetStrategy, impurity, maxDepth, maxBins, seedNum), test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePredictions(rddToPredict: RDD[LabeledPoint], model: org.apache.spark.mllib.tree.model.RandomForestModel): Array[Double] = {\n",
    "val predictions = rddToPredict.map { point => model.predict(point.features) }.collect()\n",
    "predictions\n",
    "}\n",
    "\n",
    "def getLabels(rddToPredict: RDD[LabeledPoint]): Array[Double] = {\n",
    "val labels = rddToPredict.map { point => point.label }.collect()\n",
    "labels\n",
    "}    \n",
    "\n",
    "val predictions = modelsAndTest.flatMap { case (model, test) => computePredictions(test, model) }\n",
    "val actuals = modelsAndTest.flatMap { case (model, test) => getLabels(test) }\n",
    "val predictionsAndLabels = sc.parallelize(predictions zip actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2  = 0.8541554204558702\n",
      "MAE = 2729.1208020016106\n"
     ]
    }
   ],
   "source": [
    "val metrics = new RegressionMetrics(predictionsAndLabels)\n",
    "val R2 = metrics.r2\n",
    "val MAE = metrics.meanAbsoluteError\n",
    "println(\"R2  = \" + R2)\n",
    "println(\"MAE = \" + MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
